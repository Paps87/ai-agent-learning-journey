#!/usr/bin/env python3
"""
Client LM Studio pour Phase 3 - Mini Perplexity
Communique avec LM Studio via l'API OpenAI-compatible
"""

import requests
import logging
from typing import Optional, Dict, Any, List

logger = logging.getLogger(__name__)

class LMStudioClient:
    """
    Client pour communiquer avec LM Studio
    Compatible avec l'API OpenAI
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:1234/v1",
        model: str = "gad-gpt-5-chat-llama-3.1-8b-instruct-i1",
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 120  # Augment√© √† 120s pour LM Studio
    ):
        """
        Initialise le client LM Studio
        
        Args:
            base_url: URL de base de LM Studio
            model: Nom du mod√®le √† utiliser
            temperature: Temp√©rature de g√©n√©ration (0-1)
            max_tokens: Nombre maximum de tokens
            timeout: Timeout en secondes
        """
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        
        logger.info(f"LMStudioClient initialis√©: {base_url}, mod√®le={model}")
    
    def test_connection(self) -> bool:
        """
        Teste la connexion √† LM Studio
        
        Returns:
            True si connect√©, False sinon
        """
        try:
            response = requests.get(
                f"{self.base_url}/models",
                timeout=5
            )
            response.raise_for_status()
            
            models = response.json().get("data", [])
            logger.info(f"‚úÖ LM Studio connect√©: {len(models)} mod√®les disponibles")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå LM Studio non accessible: {e}")
            return False
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """
        G√©n√®re une r√©ponse avec LM Studio
        
        Args:
            prompt: Prompt utilisateur
            system_prompt: Prompt syst√®me (optionnel)
            temperature: Override temp√©rature
            max_tokens: Override max_tokens
            
        Returns:
            R√©ponse g√©n√©r√©e
        """
        try:
            # Construire les messages
            messages = []
            
            if system_prompt:
                messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            messages.append({
                "role": "user",
                "content": prompt
            })
            
            # Param√®tres de g√©n√©ration
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature or self.temperature,
                "max_tokens": max_tokens or self.max_tokens
            }
            
            logger.debug(f"G√©n√©ration LM Studio: {len(prompt)} caract√®res")
            
            # Appel API
            response = requests.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            # Extraire la r√©ponse
            result = response.json()
            answer = result["choices"][0]["message"]["content"]
            
            logger.info(f"‚úÖ R√©ponse g√©n√©r√©e: {len(answer)} caract√®res")
            return answer
            
        except requests.exceptions.Timeout:
            logger.error("‚è∞ Timeout lors de la g√©n√©ration")
            return "D√©sol√©, la g√©n√©ration a pris trop de temps. Veuillez r√©essayer."
            
        except requests.exceptions.ConnectionError:
            logger.error("üåê LM Studio non accessible")
            return "Erreur: LM Studio n'est pas accessible. V√©rifiez qu'il est bien lanc√© sur le port 1234."
            
        except Exception as e:
            logger.error(f"üí• Erreur g√©n√©ration: {e}")
            return f"Erreur lors de la g√©n√©ration: {str(e)}"
    
    def generate_with_context(
        self,
        question: str,
        context: str,
        sources: List[Dict[str, str]]
    ) -> str:
        """
        G√©n√®re une r√©ponse avec contexte et sources
        Format optimis√© pour Mini Perplexity
        
        Args:
            question: Question de l'utilisateur
            context: Contexte extrait des recherches
            sources: Liste des sources avec titre et URL
            
        Returns:
            R√©ponse avec citations [1], [2], etc.
        """
        
        # Construire le prompt syst√®me
        system_prompt = """Tu es un assistant de recherche intelligent et pr√©cis.

R√àGLES IMPORTANTES:
1. R√©ponds UNIQUEMENT avec les informations fournies dans les sources
2. Cite TOUJOURS tes sources avec [1], [2], [3], etc.
3. Sois pr√©cis, factuel et concis
4. Si l'information n'est pas dans les sources, dis-le clairement
5. Structure ta r√©ponse de mani√®re claire avec des paragraphes
6. N'invente JAMAIS d'informations"""

        # Construire le prompt utilisateur avec sources num√©rot√©es
        sources_text = "\n\n".join([
            f"[{i+1}] {source.get('title', 'Sans titre')}\n"
            f"URL: {source.get('url', 'N/A')}\n"
            f"Contenu: {source.get('snippet', '')}"
            for i, source in enumerate(sources)
        ])
        
        user_prompt = f"""SOURCES DISPONIBLES:
{sources_text}

CONTEXTE ADDITIONNEL:
{context[:1000]}

QUESTION: {question}

R√©ponds √† la question en citant tes sources avec [1], [2], etc."""

        return self.generate(
            prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=0.3,  # Plus bas pour plus de pr√©cision
            max_tokens=1000  # R√©duit pour r√©ponses plus rapides
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques du client"""
        return {
            "base_url": self.base_url,
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "timeout": self.timeout
        }

# Instance globale
lm_studio_client = LMStudioClient()

def get_lm_studio_client() -> LMStudioClient:
    """Factory function pour l'instance globale"""
    return lm_studio_client

# Tests
if __name__ == "__main__":
    print("üß™ Test du LM Studio Client")
    print("=" * 60)
    
    # Initialisation
    client = LMStudioClient()
    
    # Test connexion
    print("\n1. Test de connexion...")
    if client.test_connection():
        print("   ‚úÖ Connexion r√©ussie")
    else:
        print("   ‚ùå Connexion √©chou√©e")
        print("   üí° V√©rifiez que LM Studio est lanc√© sur le port 1234")
        exit(1)
    
    # Test g√©n√©ration simple
    print("\n2. Test de g√©n√©ration simple...")
    response = client.generate("Dis bonjour en fran√ßais")
    print(f"   R√©ponse: {response[:100]}...")
    
    # Test avec contexte
    print("\n3. Test avec contexte et sources...")
    sources = [
        {
            "title": "Bitcoin Price Today",
            "url": "https://coinmarketcap.com/currencies/bitcoin/",
            "snippet": "The live Bitcoin price is $87,426.84 USD"
        },
        {
            "title": "Bitcoin Analysis",
            "url": "https://example.com/btc",
            "snippet": "Bitcoin shows strong momentum in Q4 2024"
        }
    ]
    
    response = client.generate_with_context(
        question="Quel est le prix du Bitcoin?",
        context="Bitcoin est une cryptomonnaie populaire.",
        sources=sources
    )
    
    print(f"   R√©ponse: {response[:200]}...")
    
    # Statistiques
    print("\n4. Statistiques:")
    stats = client.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    print("\n‚úÖ Tests termin√©s!")
