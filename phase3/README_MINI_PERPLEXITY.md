# ğŸ‰ Mini Perplexity - Guide de Lancement

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

1. **LM Studio** doit Ãªtre lancÃ© sur le port 1234
   - Ouvrir LM Studio
   - Charger le modÃ¨le GPT 8B
   - DÃ©marrer le serveur local

2. **DÃ©pendances Python** installÃ©es
   ```bash
   source venv/bin/activate
   pip install fastapi uvicorn python-multipart
   ```

### Lancement

**Terminal 1 - Backend :**
```bash
cd "/home/paps/Projet ai/phase3"
./run_backend.sh
```

**Terminal 2 - Frontend :**
```bash
cd "/home/paps/Projet ai/phase3"
./run_frontend.sh
```

**Ouvrir le navigateur :**
- Frontend : http://localhost:8080
- API Docs : http://localhost:8000/docs

---

## ğŸ“ Structure du Projet

```
phase3/
â”œâ”€â”€ src/                          # Modules Phase 3 existants
â”‚   â”œâ”€â”€ web_search.py            # âœ… Recherche DuckDuckGo
â”‚   â”œâ”€â”€ html_parser.py           # âœ… Parsing HTML
â”‚   â”œâ”€â”€ extended_rag_pipeline.py # âœ… RAG + LM Studio
â”‚   â”œâ”€â”€ agent_orchestrator.py    # âœ… Orchestrateur
â”‚   â””â”€â”€ lmstudio_client.py       # ğŸ†• Client LM Studio
â”‚
â”œâ”€â”€ backend/                      # ğŸ†• API FastAPI
â”‚   â””â”€â”€ api.py                   # Endpoints REST
â”‚
â”œâ”€â”€ frontend/                     # ğŸ†• Interface Web
â”‚   â”œâ”€â”€ index.html               # Interface moderne
â”‚   â”œâ”€â”€ style.css                # Dark mode Perplexity
â”‚   â””â”€â”€ app.js                   # Logique frontend
â”‚
â”œâ”€â”€ app/                          # Interface Streamlit (conservÃ©e)
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ run_backend.sh               # ğŸ†• Script backend
â”œâ”€â”€ run_frontend.sh              # ğŸ†• Script frontend
â””â”€â”€ test_*.py                    # Tests de validation
```

---

## ğŸ§ª Test Rapide

### 1. Tester LM Studio

```bash
cd "/home/paps/Projet ai/phase3/src"
source ../../venv/bin/activate
python lmstudio_client.py
```

**Attendu :** âœ… Connexion rÃ©ussie + gÃ©nÃ©ration avec citations

### 2. Tester le Backend

```bash
# Terminal 1: Lancer backend
cd "/home/paps/Projet ai/phase3"
./run_backend.sh

# Terminal 2: Tester API
curl -X POST http://localhost:8000/api/ask \
  -H "Content-Type: application/json" \
  -d '{"query": "prix bitcoin"}'
```

**Attendu :** JSON avec `answer`, `sources`, `processing_time`

### 3. Tester le Frontend

1. Lancer backend (voir ci-dessus)
2. Lancer frontend : `./run_frontend.sh`
3. Ouvrir http://localhost:8080
4. Poser question : "Quel est le prix du Bitcoin?"

**Attendu :**
- RÃ©ponse avec citations [1], [2]
- Sources affichÃ©es en bas
- Liens cliquables

---

## ğŸ¯ FonctionnalitÃ©s

### Backend API

**Endpoints :**
- `POST /api/ask` - Poser une question
- `GET /api/health` - VÃ©rifier statut
- `GET /api/stats` - Statistiques agent
- `GET /docs` - Documentation interactive

**Workflow :**
1. Recherche web (DuckDuckGo)
2. Parsing HTML + chunking
3. GÃ©nÃ©ration LLM (LM Studio)
4. RÃ©ponse avec citations

### Frontend

**Features :**
- ğŸ¨ Dark mode style Perplexity
- ğŸ” Recherche en temps rÃ©el
- ğŸ“š Citations inline [1], [2], [3]
- ğŸ”— Sources cliquables
- âš¡ Animations smooth
- ğŸ“± Responsive design

---

## ğŸ”§ Configuration

### LM Studio

**ModÃ¨le utilisÃ© :** `gad-gpt-5-chat-llama-3.1-8b-instruct-i1`

**ParamÃ¨tres :**
- Temperature : 0.3 (prÃ©cision)
- Max tokens : 2000
- Timeout : 60s

### Backend

**Port :** 8000
**CORS :** ActivÃ© pour localhost:8080

### Frontend

**Port :** 8080
**API URL :** http://localhost:8000

---

## ğŸ’¡ Utilisation

### Questions SuggÃ©rÃ©es

- "Quel est le prix du Bitcoin aujourd'hui?"
- "Quelles sont les derniÃ¨res actualitÃ©s sur l'IA?"
- "Comparer Python et JavaScript pour le dÃ©veloppement web"

### Citations

Les rÃ©ponses incluent des citations numÃ©rotÃ©es :
- `[1]`, `[2]`, `[3]` dans le texte
- Cliquables pour scroller vers la source
- Sources affichÃ©es en bas avec titre + URL

---

## ğŸ› DÃ©pannage

### Backend ne dÃ©marre pas

```bash
# VÃ©rifier que le venv est activÃ©
source venv/bin/activate

# RÃ©installer dÃ©pendances
pip install fastapi uvicorn
```

### LM Studio non connectÃ©

1. Ouvrir LM Studio
2. Charger un modÃ¨le
3. Cliquer "Start Server"
4. VÃ©rifier : http://localhost:1234/v1/models

### Frontend ne charge pas

1. VÃ©rifier que le backend tourne (port 8000)
2. Ouvrir la console du navigateur (F12)
3. VÃ©rifier les erreurs CORS

---

## ğŸ“Š Performance

**Temps de rÃ©ponse typique :**
- Recherche web : 1-2s
- Parsing HTML : 0.5-1s
- GÃ©nÃ©ration LLM : 3-10s
- **Total : 5-15s**

---

## ğŸ‰ Prochaines Ã‰tapes

- [ ] Ajouter streaming des rÃ©ponses (SSE)
- [ ] Historique des conversations
- [ ] Export PDF des rÃ©ponses
- [ ] Mode comparaison (2 sources)
- [ ] Support images dans rÃ©ponses

---

**CrÃ©Ã© avec â¤ï¸ - Phase 3 Mini Perplexity**
