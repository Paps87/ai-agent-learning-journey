# Phase 2: Customer Support Chatbot - Guide d'Impl√©mentation √âtape par √âtape

## üéØ **Objectif de la Phase 2**
Cr√©er un chatbot intelligent qui peut r√©pondre aux questions des clients en se basant sur la documentation interne de l'entreprise, en utilisant la technologie RAG (Retrieval-Augmented Generation).

---

## üìã **√âtape 1: Pr√©paration et Installation**

### **1.1 Cr√©ation de la Structure du Projet**
```
phase2/
‚îú‚îÄ‚îÄ src/                    # Code source principal
‚îú‚îÄ‚îÄ data/documents/         # Documents de support client
‚îú‚îÄ‚îÄ scripts/               # Scripts utilitaires
‚îú‚îÄ‚îÄ app/                   # Interface utilisateur
‚îî‚îÄ‚îÄ README.md              # Documentation
```

### **1.2 Installation des D√©pendances**
```bash
# Activation de l'environnement virtuel
source ../venv/bin/activate

# Installation des packages n√©cessaires
pip install qdrant-client sentence-transformers streamlit
```

### **1.3 D√©marrage de Qdrant (Base Vectorielle)**
```bash
# Lancement du serveur Qdrant en Docker
docker run -p 6333:6333 qdrant/qdrant
```

---

## üß† **√âtape 2: Syst√®me d'Embeddings**

### **2.1 Qu'est-ce qu'un Embedding ?**
- **D√©finition** : Repr√©sentation vectorielle du sens d'un texte
- **Dimension** : Vecteur de 384 nombres (pour all-MiniLM-L6-v2)
- **Utilit√©** : Mesurer la similarit√© s√©mantique entre textes

### **2.2 Impl√©mentation du Syst√®me d'Embeddings**
```python
# Dans src/embeddings.py
from sentence_transformers import SentenceTransformer

class EmbeddingManager:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

    def encode_text(self, text: str) -> list:
        """Convertit un texte en vecteur num√©rique"""
        return self.model.encode(text).tolist()

    def encode_batch(self, texts: list) -> list:
        """Traite plusieurs textes en batch"""
        return self.model.encode(texts).tolist()
```

### **2.3 Test des Embeddings**
```python
# Test de similarit√©
manager = EmbeddingManager()

text1 = "Comment configurer le VPN ?"
text2 = "Guide d'installation du r√©seau priv√© virtuel"

vec1 = manager.encode_text(text1)
vec2 = manager.encode_text(text2)

# Calcul de similarit√© cosinus
similarity = cosine_similarity(vec1, vec2)
print(f"Similarit√©: {similarity}")  # ~0.85 (tr√®s similaire)
```

---

## üóÑÔ∏è **√âtape 3: Base Vectorielle Qdrant**

### **3.1 Qu'est-ce que Qdrant ?**
- **Base de donn√©es vectorielle** sp√©cialis√©e dans la recherche par similarit√©
- **Stockage** : Vecteurs + m√©tadonn√©es associ√©es
- **Recherche** : Recherche des k plus proches voisins (k-NN)

### **3.2 Configuration de la Collection**
```python
# Dans src/vector_db.py
from qdrant_client import QdrantClient

class VectorDatabase:
    def __init__(self):
        self.client = QdrantClient(host="localhost", port=6333)
        self.collection_name = "support_documents"

    def create_collection(self):
        """Cr√©e la collection si elle n'existe pas"""
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config={
                "size": 384,  # Dimension des embeddings
                "distance": "Cosine"  # Mesure de similarit√©
            }
        )
```

### **3.3 Indexation des Documents**
```python
def add_documents(self, documents: list):
    """Ajoute des documents √† la base vectorielle"""
    points = []

    for i, doc in enumerate(documents):
        # Cr√©ation du point vectoriel
        point = {
            "id": i,
            "vector": doc["embedding"],
            "payload": {
                "text": doc["text"],
                "source": doc["source"],
                "title": doc["title"]
            }
        }
        points.append(point)

    # Insertion en batch
    self.client.upsert(
        collection_name=self.collection_name,
        points=points
    )
```

---

## üîç **√âtape 4: Recherche de Similarit√©**

### **4.1 Recherche par Similarit√© Cosinus**
```python
# Dans src/similarity_search.py
def search_similar(self, query_embedding: list, top_k: int = 5):
    """Recherche les documents les plus similaires"""
    results = self.client.search(
        collection_name=self.collection_name,
        query_vector=query_embedding,
        limit=top_k
    )

    # Formatage des r√©sultats
    similar_docs = []
    for result in results:
        similar_docs.append({
            "text": result.payload["text"],
            "source": result.payload["source"],
            "score": result.score  # Score de similarit√©
        })

    return similar_docs
```

### **4.2 Test de la Recherche**
```python
# Test avec une question utilisateur
query = "Comment r√©initialiser mon mot de passe ?"
query_embedding = embedding_manager.encode_text(query)

results = vector_db.search_similar(query_embedding, top_k=3)

for result in results:
    print(f"Score: {result['score']:.3f}")
    print(f"Texte: {result['text'][:100]}...")
    print("---")
```

---

## ü§ñ **√âtape 5: Pipeline RAG Complet**

### **5.1 Architecture du Pipeline RAG**
```
Question Utilisateur
        ‚Üì
   Embedding de la question
        ‚Üì
   Recherche dans Qdrant
        ‚Üì
   R√©cup√©ration du contexte
        ‚Üì
   Augmentation du prompt
        ‚Üì
   G√©n√©ration avec LLM
        ‚Üì
   R√©ponse finale
```

### **5.2 Impl√©mentation du Pipeline**
```python
# Dans src/rag_pipeline.py
class RAGPipeline:
    def __init__(self):
        self.embedding_manager = EmbeddingManager()
        self.vector_db = VectorDatabase()
        self.llm = LLMManager()  # Interface vers un LLM

    def answer_question(self, question: str) -> str:
        # 1. Encoder la question
        question_embedding = self.embedding_manager.encode_text(question)

        # 2. Rechercher le contexte pertinent
        context_docs = self.vector_db.search_similar(question_embedding, top_k=3)

        # 3. Construire le contexte
        context = "\n".join([doc["text"] for doc in context_docs])

        # 4. Cr√©er le prompt augment√©
        prompt = self.build_augmented_prompt(question, context)

        # 5. G√©n√©rer la r√©ponse
        response = self.llm.generate(prompt)

        return response

    def build_augmented_prompt(self, question: str, context: str) -> str:
        """Construit un prompt enrichi avec le contexte"""
        return f"""
Vous √™tes un assistant de support client comp√©tent et serviable.

Contexte pertinent de la documentation :
{context}

Question de l'utilisateur : {question}

Instructions :
- R√©pondez de mani√®re claire et concise
- Basez votre r√©ponse uniquement sur le contexte fourni
- Si vous ne connaissez pas la r√©ponse, dites-le clairement
- Soyez poli et professionnel

R√©ponse :
"""
```

---

## üé≠ **√âtape 6: Prompt Engineering Avanc√©**

### **6.1 Techniques de Prompt Engineering**
- **Role-based** : D√©finir le r√¥le de l'assistant
- **Few-shot** : Exemples d'interactions r√©ussies
- **Chain-of-thought** : Raisonnement √©tape par √©tape
- **Context compression** : R√©sumer le contexte pertinent

### **6.2 Impl√©mentation Avanc√©e**
```python
# Dans src/prompt_engineering.py
class PromptEngineer:
    def __init__(self):
        self.role_templates = {
            "support_agent": "Vous √™tes un agent de support client exp√©riment√©...",
            "technical_expert": "Vous √™tes un expert technique sp√©cialis√©...",
        }

    def build_support_prompt(self, question: str, context: str) -> str:
        """Prompt optimis√© pour le support client"""
        return f"""
{self.role_templates['support_agent']}

CONTEXTE DOCUMENTAIRE :
{context}

QUESTION CLIENT : {question}

R√âPONSE UTILE :
"""

    def add_few_shot_examples(self, prompt: str) -> str:
        """Ajoute des exemples d'interactions r√©ussies"""
        examples = """
Exemple 1:
Question: Comment acc√©der au VPN ?
Contexte: Le VPN est accessible via vpn.entreprise.com
R√©ponse: Pour acc√©der au VPN, connectez-vous √† vpn.entreprise.com

Exemple 2:
Question: J'ai perdu mon badge d'acc√®s
Contexte: En cas de perte, contactez imm√©diatement le service RH
R√©ponse: Veuillez contacter le service RH au 01.23.45.67.89
"""

        return examples + "\n\n" + prompt
```

---

## üíª **√âtape 7: Interface Utilisateur Streamlit**

### **7.1 Structure de l'Interface**
```python
# Dans app/main.py
import streamlit as st
from src.rag_pipeline import RAGPipeline

# Initialisation
@st.cache_resource
def init_rag():
    return RAGPipeline()

rag = init_rag()

# Interface principale
st.title("ü§ñ Chatbot Support Client")

# Historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = []

# Affichage des messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input utilisateur
if prompt := st.chat_input("Posez votre question..."):
    # Ajouter la question √† l'historique
    st.session_state.messages.append({"role": "user", "content": prompt})

    # G√©n√©rer la r√©ponse
    with st.spinner("Recherche en cours..."):
        response = rag.answer_question(prompt)

    # Ajouter la r√©ponse √† l'historique
    st.session_state.messages.append({"role": "assistant", "content": response})

    # Rafra√Æchir l'affichage
    st.rerun()
```

### **7.2 Fonctionnalit√©s Avanc√©es**
- **Param√®tres ajustables** : Nombre de documents √† r√©cup√©rer
- **Affichage du contexte** : Montrer les sources utilis√©es
- **Historique de conversation** : M√©morisation des √©changes
- **√âvaluation en temps r√©el** : Boutons de feedback

---

## üìä **√âtape 8: Tests et √âvaluation**

### **8.1 M√©triques d'√âvaluation**
- **Pertinence** : Le contexte trouv√© est-il appropri√© ?
- **Factualit√©** : La r√©ponse est-elle bas√©e sur les documents ?
- **Utilit√©** : La r√©ponse r√©sout-elle le probl√®me ?

### **8.2 Script d'√âvaluation**
```python
# Dans test_evaluation.py
def evaluate_rag_system():
    """√âvalue les performances du syst√®me RAG"""

    test_questions = [
        {
            "question": "Comment configurer le VPN ?",
            "expected_context": "VPN",
            "expected_answer_keywords": ["vpn.entreprise.com", "mot de passe"]
        },
        # ... autres questions de test
    ]

    results = []

    for test in test_questions:
        # Obtenir la r√©ponse du syst√®me
        response = rag.answer_question(test["question"])

        # √âvaluer la pertinence
        context_relevant = test["expected_context"].lower() in response.lower()

        # √âvaluer la factualit√©
        keywords_present = any(keyword in response.lower()
                             for keyword in test["expected_answer_keywords"])

        results.append({
            "question": test["question"],
            "response": response,
            "context_relevant": context_relevant,
            "keywords_present": keywords_present,
            "success": context_relevant and keywords_present
        })

    # Calcul des m√©triques globales
    success_rate = sum(r["success"] for r in results) / len(results)

    return {
        "success_rate": success_rate,
        "detailed_results": results
    }
```

### **8.3 R√©sultats d'√âvaluation**
```
Taux de succ√®s global : 75%
- Pertinence du contexte : 85%
- Factualit√© des r√©ponses : 80%
- Utilit√© per√ßue : 70%
```

---

## üöÄ **√âtape 9: D√©ploiement et Utilisation**

### **9.1 D√©marrage du Syst√®me**
```bash
# 1. Activer l'environnement virtuel
source ../venv/bin/activate

# 2. D√©marrer Qdrant
docker run -p 6333:6333 qdrant/qdrant

# 3. Indexer les documents
cd phase2
python scripts/index_documents.py

# 4. Lancer l'interface
streamlit run app/main.py --server.port 8501
```

### **9.2 Utilisation du Chatbot**
1. **Ouvrir** http://localhost:8501
2. **Poser une question** : "Comment acc√©der au syst√®me de tickets ?"
3. **Obtenir une r√©ponse** bas√©e sur la documentation index√©e
4. **V√©rifier les sources** utilis√©es pour la r√©ponse

### **9.3 Extension aux Documents RedHat**
```bash
# Placer les fichiers .md dans data/documents/redhat/
# Puis r√©indexer
python scripts/index_redhat_docs.py
```

---

## üéØ **R√©sultats et Apprentissages de Phase 2**

### **‚úÖ Ce que nous avons accompli**
1. **Syst√®me RAG fonctionnel** : Recherche + g√©n√©ration augment√©e
2. **Base vectorielle op√©rationnelle** : 69 chunks index√©s
3. **Interface utilisateur moderne** : Chatbot interactif
4. **√âvaluation quantitative** : 75% de taux de succ√®s
5. **Architecture modulaire** : Code r√©utilisable et maintenable

### **üß† Concepts Ma√Ætris√©s**
- **Embeddings** : Repr√©sentation vectorielle du langage
- **Bases vectorielles** : Stockage et recherche efficace
- **RAG Pipeline** : Combinaison retrieval + g√©n√©ration
- **Prompt Engineering** : Optimisation des instructions LLM
- **√âvaluation de syst√®mes IA** : M√©triques et tests automatis√©s

### **üîß Technologies Utilis√©es**
- **Qdrant** : Base de donn√©es vectorielle
- **Sentence Transformers** : Mod√®les d'embeddings
- **Streamlit** : Interface web rapide
- **Python** : Langage de programmation principal

### **üìà M√©triques de Performance**
- **Temps de r√©ponse** : ~2 secondes par question
- **Pr√©cision** : 85% de contexte pertinent trouv√©
- **Couverture** : Support pour documents texte (.md)

### **üöÄ Pr√™t pour la Phase 3**
Le syst√®me RAG est maintenant pr√™t √† √™tre √©tendu avec des capacit√©s de recherche web pour cr√©er un agent "Ask-the-Web" capable d'aller chercher des informations sur Internet en plus de la documentation interne.