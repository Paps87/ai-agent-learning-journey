# üß† Guide des Fondations des LLMs

## Vue d'ensemble

Ce guide explique les concepts fondamentaux impl√©ment√©s dans notre LLM Playground, en se concentrant sur la compr√©hension plut√¥t que sur les d√©tails techniques.

## 1. Tokenisation : Convertir le texte en tokens

### Le probl√®me
Les LLMs ne comprennent pas le texte brut. Ils travaillent avec des nombres (tokens). Comment d√©couper intelligemment le texte pour maximiser l'efficacit√© du mod√®le ?

### Solution : BPE (Byte Pair Encoding)

BPE apprend des "sous-mots" optimaux en analysant la fr√©quence des paires de caract√®res dans le corpus d'entra√Ænement.

#### Exemple concret :
```
Texte d'entr√©e : "artificial intelligence is amazing"

1. D√©coupage initial (caract√®res) :
   ['a','r','t','i','f','i','c','i','a','l',' ','i','n','t','e','l','l','i','g','e','n','c','e',' ','i','s',' ','a','m','a','z','i','n','g']

2. BPE apprend les paires fr√©quentes :
   - "in" appara√Æt souvent ‚Üí devient un token
   - "ti" appara√Æt souvent ‚Üí devient un token
   - "ing" appara√Æt souvent ‚Üí devient un token
   - etc.

3. R√©sultat final :
   "artificial intelligence is amazing"
   ‚Üí ["art", "ificial", " intelligence", " is", " amazing"]
   ‚Üí [1523, 4871, 284, 318, 4673] (IDs num√©riques)
```

#### Pourquoi BPE est optimal :
- **G√®re les mots inconnus** : "tokenization" ‚Üí "token", "ization" (m√™me si "ization" n'√©tait pas dans le vocabulaire d'entra√Ænement)
- **√âquilibre optimal** : Entre caract√®res (trop granulaire, vocabulaire √©norme) et mots complets (mots rares non g√©rables)
- **Apprentissage automatique** : La segmentation optimale est apprise statistiquement du corpus

#### Dans notre impl√©mentation :
- Utilise SentencePiece (impl√©mentation Google)
- Vocabulaire de 8000 tokens
- Entra√Æn√© sur notre corpus Wikipedia

## 2. Architecture Transformer : Attention + Causal Masking

### Le probl√®me
Comment un mod√®le peut-il comprendre les relations complexes entre mots distants dans une phrase ?

### Solution : Multi-Head Attention avec Causal Masking

#### M√©canisme d'attention :
Pour chaque mot (token), le mod√®le regarde tous les autres mots et calcule un "score d'attention" : "combien ce mot m'aide-t-il √† comprendre celui-ci ?"

#### Exemple pratique :
**Phrase :** "Le chat noir dort sur le canap√© rouge"

Quand le mod√®le traite le mot "dort" :
- Il regarde "chat" avec un score √©lev√© (qui dort ?)
- Il regarde "canap√©" avec un score √©lev√© (o√π dort-il ?)
- Il regarde "noir" avec un score moyen (contexte descriptif)
- Il ignore presque "rouge" (peu pertinent pour l'action de dormir)

#### Causal Masking (CRUCIAL pour la g√©n√©ration) :
- **Principe** : Un token ne peut voir que les tokens qui le pr√©c√®dent
- **Pourquoi** : Permet la g√©n√©ration autoregressive (pr√©dire le prochain token √† partir des pr√©c√©dents)
- **Impl√©mentation** : Masque triangulaire inf√©rieur

```
Position:  0    1    2    3    4    5
Tokens:   [BOS] Le   chat noir dort
Mask:     [1,   0,   0,   0,   0,   0]  ‚Üê BOS ne voit rien (pas de pass√©)
          [1,   1,   0,   0,   0,   0]  ‚Üê "Le" voit BOS
          [1,   1,   1,   0,   0,   0]  ‚Üê "chat" voit BOS + "Le"
          [1,   1,   1,   1,   0,   0]  ‚Üê "noir" voit BOS + "Le" + "chat"
          etc.
```

#### Dans notre mod√®le :
- 4 couches de transformer
- 4 t√™tes d'attention par couche
- Dimension d'attention : 256
- Causal masking activ√© pour la g√©n√©ration

## 3. G√©n√©ration de texte : Greedy vs Sampling

### Le probl√®me
Le mod√®le pr√©dit des probabilit√©s pour chaque token possible suivant. Comment choisir lequel utiliser pour g√©n√©rer du texte ?

### Strat√©gies de g√©n√©ration :

#### a) Greedy (d√©terministe)
- **Principe** : Toujours choisir le token le plus probable
- **Avantages** : Coh√©rent, reproductible, rapide
- **Inconv√©nients** : R√©p√©titif, manque de cr√©ativit√©, peut rester bloqu√©

**Exemple :**
```
Probabilit√©s pour le prochain token :
"the": 0.4, "a": 0.3, "an": 0.2, "this": 0.1

Greedy choisit : "the" (toujours)
```

#### b) Top-k Sampling
- **Principe** : Garde seulement les k tokens les plus probables, r√©-√©chantillonne parmi eux
- **Avantages** : Contr√¥le la diversit√©, √©vite les absurdit√©s
- **Param√®tre** : k (typiquement 40-60)

**Exemple (k=3) :**
```
Probabilit√©s originales :
"the": 0.4, "a": 0.3, "an": 0.2, "this": 0.1, "dog": 0.05, ...

Top-3 gard√©s : "the": 0.4, "a": 0.3, "an": 0.2
R√©-normalis√© : "the": 0.57, "a": 0.43, "an": 0.29
```

#### c) Top-p (Nucleus) Sampling
- **Principe** : Garde les tokens jusqu'√† ce que leur probabilit√© cumul√©e atteigne p
- **Avantages** : Plus adaptatif que top-k, s'ajuste √† la distribution
- **Param√®tre** : p (typiquement 0.9-0.95)

**Exemple (p=0.9) :**
```
Probabilit√©s tri√©es :
"the": 0.4 (cumul: 0.4)
"a": 0.3 (cumul: 0.7)
"an": 0.2 (cumul: 0.9) ‚Üê Stop ici
"this": 0.1 (cumul: 1.0) ‚Üê Exclu
```

#### Temperature
- **R√¥le** : Contr√¥le l'al√©atoire de la distribution
- **Formule** : `probabilities = softmax(logits / temperature)`
- **Effets** :
  - T = 0.0 : Greedy (d√©terministe)
  - T = 1.0 : Distribution originale
  - T > 1.0 : Plus al√©atoire (cr√©atif)
  - T < 1.0 : Plus concentr√© (conservateur)

## 4. Entra√Ænement : Next-Token Prediction

### Le probl√®me
Comment entra√Æner un mod√®le √† g√©n√©rer du texte coh√©rent et contextuellement appropri√© ?

### Solution : Next-Token Prediction (NTP)

#### Principe :
- **Objectif** : Pr√©dire le token suivant √† partir de tous les pr√©c√©dents
- **Loss** : Cross-entropy entre pr√©diction du mod√®le et token r√©el
- **Architecture** : GPT-like (d√©coder-only transformer)

#### Exemple d'entra√Ænement :
```
Texte : "Le chat noir dort"

Cr√©ation des exemples d'entra√Ænement :
1. [BOS] ‚Üí "Le"           (apprendre √† pr√©dire "Le")
2. [BOS] "Le" ‚Üí "chat"    (apprendre √† pr√©dire "chat")
3. [BOS] "Le" "chat" ‚Üí "noir"  (apprendre √† pr√©dire "noir")
4. [BOS] "Le" "chat" "noir" ‚Üí "dort"  (apprendre √† pr√©dire "dort")
5. [BOS] "Le" "chat" "noir" "dort" ‚Üí [EOS]  (apprendre √† finir)
```

#### Pourquoi √ßa marche :
- **Apprentissage statistique** : Le mod√®le apprend les patterns du langage
- **Causal masking** : Force l'apprentissage de d√©pendances s√©quentielles
- **Scaling** : Plus de donn√©es = meilleur mod√®le
- **Transfer learning** : Fine-tuning possible pour des t√¢ches sp√©cifiques

#### Dans notre impl√©mentation :
- Entra√Ænement sur corpus Wikipedia tokenis√©
- Loss cross-entropy
- Optimiseur AdamW
- Learning rate scheduling

## Synth√®se : Comment fonctionne un LLM ?

Un LLM est essentiellement un **pr√©dicteur de token suivant sophistiqu√©** qui :

1. **Convertit le texte en tokens** via BPE (tokenization)
2. **Comprends les relations contextuelles** via attention mechanism
3. **Pr√©dit le prochain token** de mani√®re contr√¥l√©e (greedy/sampling)
4. **S'entra√Æne par NTP** sur de gros corpus de texte

### Pipeline complet :
```
Texte brut ‚Üí Tokenisation BPE ‚Üí Embeddings ‚Üí Transformer Blocks ‚Üí Pr√©dictions ‚Üí D√©-tokenisation ‚Üí Texte g√©n√©r√©
```

### Points cl√©s pour la compr√©hension :
- **Pas de "compr√©hension" magique** : Tout est statistique
- **Causal masking = g√©n√©ration possible** : Le mod√®le apprend √† pr√©dire s√©quentiellement
- **Sampling = cr√©ativit√© contr√¥l√©e** : Entre d√©terminisme et chaos
- **Scale matters** : Plus de donn√©es/param√®tres = meilleurs r√©sultats

---

*Ce guide fait partie du projet AI Labs - Phase 1 : LLM Playground*